{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f183ce",
   "metadata": {},
   "source": [
    "# Sectora Horizon\n",
    "---\n",
    "A Future-Focused Analytics System for Industries and Busisness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1038a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingest.catalog import DatasetCatalog\n",
    "from ingest.loader import RawDatasetLoader\n",
    "from utils.uuid import generate_deterministic_id_name_based\n",
    "from utils.clean import clean_text, is_numeric_string, normalize_code_to_length, normalize_text\n",
    "from ingest.fetch.csv import CsvAdapter\n",
    "from ingest.fetch.sct import SocrataAdapter\n",
    "from ingest.fetch.excel import ExcelAdapter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "catalog = DatasetCatalog()\n",
    "loader = RawDatasetLoader(\n",
    "    csv_adapter=CsvAdapter(),\n",
    "    sct_adapter=SocrataAdapter(),\n",
    "    excel_adapter=ExcelAdapter(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378ea3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = catalog.get(\"master_csv\")\n",
    "records = list(loader.load(ds))\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "830ef7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPI columns: ['year', 'CPI']\n"
     ]
    }
   ],
   "source": [
    "# El CSV debe tener columnas: 'year' y 'CPI' (índice anual)\n",
    "cpi_ds = catalog.get(\"IPCanal\")\n",
    "records = list(loader.load(cpi_ds))\n",
    "cpi = pd.DataFrame(records)\n",
    "cpi\n",
    "\n",
    "# normalizar nombres de columnas comunes\n",
    "if 'Año de Corte'  in cpi.columns and 'year' not in cpi.columns:\n",
    "    cpi = cpi.rename(columns={'Año de Corte':'year'})\n",
    "if 'CPI' not in cpi.columns:\n",
    "    # intentar detectar la columna de índice de precios\n",
    "    possible = [c for c in cpi.columns if 'cpi' in c.lower() or 'ipc' in c.lower() or 'indice' in c.lower()]\n",
    "    if possible:\n",
    "        cpi = cpi.rename(columns={possible[0]:'CPI'})\n",
    "cpi.columns = cpi.columns.map(lambda x: x.encode('utf-8').decode('utf-8-sig').strip())\n",
    "print('CPI columns:', cpi.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb843a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df Año de Corte dtype (normalizado): Int64\n",
      "CPI Año de Corte dtype (normalizado): Int64\n",
      "Años en CPI: [2020, 2021, 2022, 2023, 2024]\n",
      "Filas df con year inválido: 0\n",
      "Filas cpi con Año de Corte inválido: 0\n",
      "0     1.61\n",
      "1     5.62\n",
      "2    13.12\n",
      "3     9,28\n",
      "4      5.2\n",
      "Name: CPI, dtype: object\n",
      "Missing CPI after merge: 0 rows\n",
      "Post-merge missing CPI: 0 rows\n"
     ]
    }
   ],
   "source": [
    "# Asegurarnos de tener columna 'Año de Corte' en cpi\n",
    "if 'year' not in cpi.columns:\n",
    "    if 'Año de Corte' in cpi.columns:\n",
    "        cpi = cpi.rename(columns={'Año de Corte':'year'})\n",
    "    else:\n",
    "        # intentar detectar columna con años (4 dígitos)\n",
    "        for col in cpi.columns:\n",
    "            sample = cpi[col].astype(str).str.extract(r'(\\d{4})', expand=False)\n",
    "            if sample.dropna().shape[0] > 0:\n",
    "                cpi = cpi.rename(columns={col:'year'})\n",
    "                \n",
    "                break\n",
    "\n",
    "# Convertir ambos a numérico entero (manejar strings, espacios, comas, separadores de miles)\n",
    "# Limpiar: eliminar cualquier caracter no numérico, luego convertir a número\n",
    "\n",
    "def clean_year_series(s):\n",
    "    s2 = s.astype(str).str.strip().replace({'': pd.NA})\n",
    "    # eliminar todo lo que no sea dígito\n",
    "    s2 = s2.str.replace(r'[^0-9]', '', regex=True)\n",
    "    # si quedó vacío, convertir a NA\n",
    "    s2 = s2.replace('', pd.NA)\n",
    "    return pd.to_numeric(s2, errors='coerce').astype('Int64')\n",
    "\n",
    "# Aplicar limpieza\n",
    "if 'year' in df.columns:\n",
    "    df['year'] = clean_year_series(df['year'])\n",
    "else:\n",
    "    raise KeyError(\"La columna 'year' no existe en df\")\n",
    "\n",
    "cpi['year'] = clean_year_series(cpi['year'])\n",
    "\n",
    "print(\"df Año de Corte dtype (normalizado):\", df['year'].dtype)\n",
    "print(\"CPI Año de Corte dtype (normalizado):\", cpi['year'].dtype)\n",
    "print(\"Años en CPI:\", sorted(cpi['year'].dropna().unique()))\n",
    "\n",
    "# Identificar filas que no pudieron convertirse (muestra algunas muestras para diagnóstico)\n",
    "bad_df_years = df[df['year'].isna()].head(10)\n",
    "print(\"Filas df con year inválido:\", df['year'].isna().sum())\n",
    "if not bad_df_years.empty:\n",
    "    print(\"Ejemplos (valores originales):\")\n",
    "    print(bad_df_years.head(10))\n",
    "print(\"Filas cpi con Año de Corte inválido:\", cpi['year'].isna().sum())\n",
    "print(cpi['CPI'])\n",
    "# Ahora hacer el merge de forma segura\n",
    "df = df.merge(cpi[['year', 'CPI']], on='year', how='left')\n",
    "missing_cpi = int(df['CPI'].isnull().sum())\n",
    "print(f'Missing CPI after merge: {missing_cpi} rows')\n",
    "\n",
    "# --- Celda agregada: diagnóstico e imputación de CPI por año más cercano ---\n",
    "import numpy as np\n",
    "# Diagnóstico adicional: identificar años en df que no aparecen en cpi y exportar muestras\n",
    "missing = int(df['CPI'].isna().sum())\n",
    "print(f'Post-merge missing CPI: {missing} rows')\n",
    "if missing > 0:\n",
    "    df_years = pd.Series(df['Año de Corte'].dropna().unique()).astype('Int64')\n",
    "    cpi_years = pd.Series(cpi['Año de Corte'].dropna().unique()).astype('Int64')\n",
    "    years_not_in_cpi = sorted(list(set(df_years.dropna().astype(int).tolist()) - set(cpi_years.dropna().astype(int).tolist())))\n",
    "    print('Unique df years not in CPI (sample):', years_not_in_cpi[:20])\n",
    "    print('CPI available years:', sorted(cpi_years.dropna().astype(int).tolist()))\n",
    "    print('Counts per df year where CPI is missing:')\n",
    "    print(df[df['CPI'].isna()]['Año de Corte'].value_counts().sort_index())\n",
    "    display(df[df['CPI'].isna()].head(20))\n",
    "    # Intentar imputar CPI usando el año más cercano disponible en la serie CPI\n",
    "    cpi_map = {int(y): float(v) for y, v in zip(cpi['Año de Corte'], cpi['CPI']) if pd.notna(y) and pd.notna(v)}\n",
    "    if len(cpi_map) > 0:\n",
    "        cpi_years_sorted = sorted(cpi_map.keys())\n",
    "        def nearest_cpi(val):\n",
    "            if pd.isna(val):\n",
    "                return pd.NA\n",
    "            try:\n",
    "                y = int(val)\n",
    "            except Exception:\n",
    "                return pd.NA\n",
    "            if y in cpi_map:\n",
    "                return cpi_map[y]\n",
    "            # buscar año más cercano\n",
    "            diffs = [abs(y - cy) for cy in cpi_years_sorted]\n",
    "            idx = int(np.argmin(diffs))\n",
    "            return cpi_map[cpi_years_sorted[idx]]\n",
    "        # Aplicar imputación solo a filas que quedaron sin CPI\n",
    "        mask_missing = df['CPI'].isna() & df['Año de Corte'].notna()\n",
    "        if mask_missing.any():\n",
    "            df.loc[mask_missing, 'CPI'] = df.loc[mask_missing, 'Año de Corte'].apply(nearest_cpi)\n",
    "            print('After nearest-year fill, missing CPI:', int(df['CPI'].isna().sum()))\n",
    "    # Exportar filas que aún queden sin CPI para revisión manual\n",
    "    still_missing = df[df['CPI'].isna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f5b9ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajustadas 40000 filas de GANANCIA a precios del año 2024.\n"
     ]
    }
   ],
   "source": [
    "# 2) Ajustar GANANCIA por inflación a precios del año base (último año disponible)\n",
    "# fix data type if required\n",
    "if df['ganancias'].dtype in [str, object]:\n",
    "    df['ganancias'] = pd.to_numeric(df['ganancias'], errors='coerce').astype('Int64')\n",
    "    df['ganancias'] = df['ganancias'].fillna(0)\n",
    "    df['ganancias'] = df['ganancias'].astype(int)\n",
    "\n",
    "if df['ingresos'].dtype in [str, object]:\n",
    "    df['ingresos'] = pd.to_numeric(df['ingresos'], errors='coerce').astype('Int64')\n",
    "    df['ingresos'] = df['ingresos'].fillna(0)\n",
    "    df['ingresos'] = df['ingresos'].astype(int)\n",
    "\n",
    "\n",
    "if 'CPI' in df.columns and df['CPI'].notna().any():\n",
    "    # Limpiar CPI: convertir strings con comas a float (ej. '9,28' -> 9.28)\n",
    "    def clean_cpi_value(val):\n",
    "        if pd.isna(val):\n",
    "            return pd.NA\n",
    "        try:\n",
    "            s = str(val).strip().replace(',', '.')\n",
    "            return float(s)\n",
    "        except Exception:\n",
    "            return pd.NA\n",
    "    df['CPI'] = df['CPI'].apply(clean_cpi_value)\n",
    "    base_year = df['year'].max()\n",
    "    base_cpi = df.loc[df['year'] == base_year, 'CPI'].dropna().unique()\n",
    "    if len(base_cpi)==0:\n",
    "        raise ValueError(f'No hay CPI para el año base {base_year}. Revisa el CSV de CPI.')\n",
    "    base_cpi = float(clean_cpi_value(base_cpi[0]))\n",
    "    df['ganancias'] = df['ganancias'] * (base_cpi / df['CPI'])\n",
    "    n_adjusted = int(df['ganancias'].notna().sum())\n",
    "    print(f\"Ajustadas {n_adjusted} filas de GANANCIA a precios del año {base_year}.\")\n",
    "else:\n",
    "    print('No se puede ajustar por inflación: falta columna CPI.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea8324eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lags y target creados.\n",
      "  Filas con ganancias_next válido: 26024\n",
      "  Filas con ganancia_lag_1 válido: 26024\n",
      "  Filas con al menos 1 lag: 26024\n",
      "  Distribución de n_ganancia_lags:\n",
      "n_ganancia_lags\n",
      "0    13976\n",
      "1    11026\n",
      "2     8442\n",
      "3     6556\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 3) Construir target t+1 y lags por NIT (ignorar gaps entre años)\n",
    "df = df.sort_values(['nit','year']).reset_index(drop=True)\n",
    "\n",
    "# target: GANANCIA_REAL del siguiente año disponible para cada NIT (ignora gaps)\n",
    "df['ganancias_next'] = df.groupby('nit')['ganancias'].shift(-1)\n",
    "\n",
    "# crear lags para ganacias e INGRESOS OPERACIONALES (todos los registros previos de la empresa)\n",
    "for lag in range(1, 5):\n",
    "    df[f'ganancia_lag_{lag}'] = df.groupby('nit')['ganancias'].shift(lag)\n",
    "    df[f'ingresos_lag_{lag}'] = df.groupby('nit')['ingresos'].shift(lag)\n",
    "\n",
    "# contar lags disponibles y crear indicador\n",
    "df['n_ganancia_lags'] = df[[f'ganancia_lag_{i}' for i in range(1,5)]].notna().sum(axis=1)\n",
    "df['n_ingresos_lags'] = df[[f'ingresos_lag_{i}' for i in range(1,5)]].notna().sum(axis=1)\n",
    "\n",
    "# features adicionales: tasa de crecimiento 1 año (si existe lag_1) y media movil de 3 años\n",
    "df['ganancia_grow_1y'] = df['ganancias'] / df['ganancia_lag_1'] - 1\n",
    "df['ganancia_roll_mean_3'] = df.groupby('nit')['ganancias'].transform(lambda s: s.shift(1).rolling(3, min_periods=1).mean())\n",
    "\n",
    "# preservar flag de outlier ya marcado\n",
    "if 'is_outlier_kde' not in df.columns:\n",
    "    df['is_outlier_kde'] = False\n",
    "\n",
    "# Diagnóstico de target y lags\n",
    "n_with_target = df['ganancias_next'].notna().sum()\n",
    "n_with_lag1 = df['ganancia_lag_1'].notna().sum()\n",
    "n_with_any_lag = (df['n_ganancia_lags'] > 0).sum()\n",
    "\n",
    "print(f'Lags y target creados.')\n",
    "print(f'  Filas con ganancias_next válido: {n_with_target}')\n",
    "print(f'  Filas con ganancia_lag_1 válido: {n_with_lag1}')\n",
    "print(f'  Filas con al menos 1 lag: {n_with_any_lag}')\n",
    "print(f'  Distribución de n_ganancia_lags:')\n",
    "print(df['n_ganancia_lags'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c00221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas disponibles para entrenamiento/evaluación: 26024\n"
     ]
    }
   ],
   "source": [
    "# 4) Preparar dataset para modelado (no excluimos empresas; solo filas sin target se ignoran en entrenamiento)\n",
    "feature_cols = [f'ganancia_lag_{i}' for i in range(1,5)] + [f'ingresos_lag_{i}' for i in range(1,5)] + ['n_ganancia_lags','n_ingresos_lags','ganancia_roll_mean_3','is_outlier_kde']\n",
    "# añadir algunas columnas numéricas originales si interesa\n",
    "for add in ['activos','pasivos','patrimonio']:\n",
    "    if add in df.columns:\n",
    "        feature_cols.append(add)\n",
    "\n",
    "df_model = df[df['ganancias_next'].notna()].copy()  # filas con target t+1 disponibles\n",
    "print('Filas disponibles para entrenamiento/evaluación:', df_model.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a42ca",
   "metadata": {},
   "source": [
    "#### Fisrt train with main features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ac1fb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping all-empty feature columns (no observed values): ['ganancia_lag_3', 'ganancia_lag_4', 'ingresos_lag_3', 'ingresos_lag_4']\n",
      "Final dataset for training: 26024 rows, 12 features\n",
      "Random split 80/20: Train 20819, Test 5205\n",
      "LightGBM not available or failed — falling back to RandomForest with median imputation: libgomp.so.1: cannot open shared object file: No such file or directory\n",
      "Trained RandomForest with median imputation\n",
      "Test RMSE: 4.92, MAE: 0.72\n",
      "\n",
      "--- K-fold Cross-Validation (5 folds) ---\n",
      "Fold 1: RMSE 4.91, MAE 0.71\n",
      "Fold 2: RMSE 9.02, MAE 0.92\n",
      "Fold 3: RMSE 8.09, MAE 1.00\n",
      "Fold 4: RMSE 14.77, MAE 1.00\n",
      "Fold 5: RMSE 5.35, MAE 0.74\n",
      "\n",
      "CV Mean RMSE: 8.43 (+/- 3.54)\n",
      "CV Mean MAE: 0.87 (+/- 0.12)\n"
     ]
    }
   ],
   "source": [
    "# 5) Entrenamiento actualizado: quitar features sin datos, añadir resúmenes y preferir LightGBM sin imputación\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import numpy as np\n",
    "import traceback\n",
    "\n",
    "# 5.1) Detectar y eliminar columnas que están completamente vacías en df_model (p. ej. lag_3/4 si no existen)\n",
    "all_na_cols = [c for c in feature_cols if c in df_model.columns and df_model[c].isna().all()]\n",
    "if all_na_cols:\n",
    "    print('Dropping all-empty feature columns (no observed values):', all_na_cols)\n",
    "    feature_cols = [c for c in feature_cols if c not in all_na_cols]\n",
    "\n",
    "# 5.2) Añadir feature resumen: último GANANCIA_REAL observada (lag 1 simple)\n",
    "df_model = df_model.sort_values(['nit','year']).reset_index(drop=True)\n",
    "df_model['last_ganancia'] = df_model.groupby('nit')['ganancias'].shift(1)\n",
    "if 'last_ganancia' not in feature_cols:\n",
    "    feature_cols.append('last_ganancia')\n",
    "if 'n_ganancia_lags' not in feature_cols:\n",
    "    feature_cols.append('n_ganancia_lags')\n",
    "\n",
    "# 5.3) Preparar X, y con tipos numéricos\n",
    "X = df_model[feature_cols].copy()\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'bool':\n",
    "        X[col] = X[col].astype(int)\n",
    "    else:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "y = pd.to_numeric(df_model['ganancias'], errors='coerce')\n",
    "\n",
    "print(f'Final dataset for training: {X.shape[0]} rows, {X.shape[1]} features')\n",
    "\n",
    "# 5.3.1) Estrategia A: Random 80/20 split — prefer LightGBM which accepts NaNs nativamente\n",
    "if X.shape[0] >= 100:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f'Random split 80/20: Train {X_train.shape[0]}, Test {X_test.shape[0]}')\n",
    "\n",
    "    preds = None\n",
    "    try:\n",
    "        import lightgbm as lgb\n",
    "        dtrain = lgb.Dataset(X_train.values, label=y_train.values)\n",
    "        params = {'objective':'regression','metric':'rmse','verbosity':-1}\n",
    "        bst = lgb.train(params, dtrain, num_boost_round=200)\n",
    "        preds = bst.predict(X_test.values)\n",
    "        print('Trained LightGBM (no global imputation)')\n",
    "    except Exception as e:\n",
    "        print('LightGBM not available or failed — falling back to RandomForest with median imputation:', e)\n",
    "        try:\n",
    "            imp = SimpleImputer(strategy='median')\n",
    "            X_train_imp = imp.fit_transform(X_train)\n",
    "            X_test_imp = imp.transform(X_test)\n",
    "            rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "            rf.fit(X_train_imp, y_train.values)\n",
    "            preds = rf.predict(X_test_imp)\n",
    "            print('Trained RandomForest with median imputation')\n",
    "        except Exception as e2:\n",
    "            print('RandomForest fallback failed:', e2)\n",
    "            traceback.print_exc()\n",
    "            preds = np.full(shape=(X_test.shape[0],), fill_value=np.nan)\n",
    "\n",
    "    if preds is not None:\n",
    "        preds = np.asarray(preds)\n",
    "        mask_ok = ~np.isnan(preds) & ~np.isnan(y_test.values)\n",
    "        if mask_ok.sum() > 0:\n",
    "            rmse = np.sqrt(mean_squared_error(y_test.values[mask_ok], preds[mask_ok]))\n",
    "            mae = mean_absolute_error(y_test.values[mask_ok], preds[mask_ok])\n",
    "            print(f'Test RMSE: {rmse:.2f}, MAE: {mae:.2f}')\n",
    "else:\n",
    "    print('Pocas filas para split 80/20 — saltando a CV')\n",
    "\n",
    "# 5.3.2) Estrategia B: K-fold CV (5 folds) — try LightGBM per-fold using raw arrays (NaNs allowed)\n",
    "print('\\n--- K-fold Cross-Validation (5 folds) ---')\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_rmses, cv_maes = [], []\n",
    "fold_num = 0\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    fold_num += 1\n",
    "    X_tr_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_tr_fold, y_val_fold = y.values[train_idx], y.values[val_idx]\n",
    "    preds_fold = None\n",
    "    # Prefer LightGBM (no imputation)\n",
    "    try:\n",
    "        import lightgbm as lgb\n",
    "        dtrain_fold = lgb.Dataset(X_tr_fold.values, label=y_tr_fold)\n",
    "        params = {'objective':'regression','metric':'rmse','verbosity':-1}\n",
    "        bst_fold = lgb.train(params, dtrain_fold, num_boost_round=200)\n",
    "        preds_fold = bst_fold.predict(X_val_fold.values)\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            imp = SimpleImputer(strategy='median')\n",
    "            X_tr_imp = imp.fit_transform(X_tr_fold)\n",
    "            X_val_imp = imp.transform(X_val_fold)\n",
    "            rf_fold = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "            rf_fold.fit(X_tr_imp, y_tr_fold)\n",
    "            preds_fold = rf_fold.predict(X_val_imp)\n",
    "        except Exception as e2:\n",
    "            print(f'Fold {fold_num} fallback error: {e2}')\n",
    "            preds_fold = None\n",
    "    if preds_fold is not None:\n",
    "        preds_fold = np.asarray(preds_fold)\n",
    "        mask_ok_fold = ~np.isnan(preds_fold) & ~np.isnan(y_val_fold)\n",
    "        if mask_ok_fold.sum() > 0:\n",
    "            rmse_fold = np.sqrt(mean_squared_error(y_val_fold[mask_ok_fold], preds_fold[mask_ok_fold]))\n",
    "            mae_fold = mean_absolute_error(y_val_fold[mask_ok_fold], preds_fold[mask_ok_fold])\n",
    "            cv_rmses.append(rmse_fold)\n",
    "            cv_maes.append(mae_fold)\n",
    "            print(f'Fold {fold_num}: RMSE {rmse_fold:.2f}, MAE {mae_fold:.2f}')\n",
    "\n",
    "if cv_rmses:\n",
    "    print(f'\\nCV Mean RMSE: {np.mean(cv_rmses):.2f} (+/- {np.std(cv_rmses):.2f})')\n",
    "    print(f'CV Mean MAE: {np.mean(cv_maes):.2f} (+/- {np.std(cv_maes):.2f})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3ed312",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15d7c060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== IMPROVED CROSS-VALIDATION: GroupKFold by NIT ===\n",
      "This ensures each fold tests on unseen companies (true generalization test)\n",
      "Fold 1: RMSE 4.86, MAE 0.78 | Train NITs: 8821, Val NITs: 2205\n",
      "Fold 2: RMSE 7.87, MAE 0.95 | Train NITs: 8821, Val NITs: 2205\n",
      "Fold 3: RMSE 27.62, MAE 1.28 | Train NITs: 8821, Val NITs: 2205\n",
      "Fold 4: RMSE 9.73, MAE 1.01 | Train NITs: 8820, Val NITs: 2206\n",
      "Fold 5: RMSE 6.81, MAE 0.76 | Train NITs: 8821, Val NITs: 2205\n",
      "\n",
      "✓ GroupKFold CV Mean RMSE: 11.38 (+/- 8.27)\n",
      "✓ GroupKFold CV Mean MAE: 0.96 (+/- 0.19)\n"
     ]
    }
   ],
   "source": [
    "# 6.1) Use GroupKFold to prevent same company in train & validation\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "print('=== IMPROVED CROSS-VALIDATION: GroupKFold by NIT ===')\n",
    "print('This ensures each fold tests on unseen companies (true generalization test)')\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "gkf_rmses, gkf_maes = [], []\n",
    "fold_num = 0\n",
    "\n",
    "for train_idx, val_idx in gkf.split(X, groups=df_model['nit'].values):\n",
    "    fold_num += 1\n",
    "    X_tr_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_tr_fold, y_val_fold = y.values[train_idx], y.values[val_idx]\n",
    "    \n",
    "    # Get unique companies in this fold\n",
    "    train_nits = df_model.iloc[train_idx]['nit'].nunique()\n",
    "    val_nits = df_model.iloc[val_idx]['nit'].nunique()\n",
    "    \n",
    "    try:\n",
    "        import lightgbm as lgb\n",
    "        dtrain_fold = lgb.Dataset(X_tr_fold.values, label=y_tr_fold)\n",
    "        params = {'objective':'regression','metric':'rmse','verbosity':-1, 'num_leaves':31, 'learning_rate':0.05}\n",
    "        bst_fold = lgb.train(params, dtrain_fold, num_boost_round=300)\n",
    "        preds_fold = bst_fold.predict(X_val_fold.values)\n",
    "    except Exception as e:\n",
    "        imp = SimpleImputer(strategy='median')\n",
    "        X_tr_imp = imp.fit_transform(X_tr_fold)\n",
    "        X_val_imp = imp.transform(X_val_fold)\n",
    "        rf_fold = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        rf_fold.fit(X_tr_imp, y_tr_fold)\n",
    "        preds_fold = rf_fold.predict(X_val_imp)\n",
    "    \n",
    "    if preds_fold is not None:\n",
    "        preds_fold = np.asarray(preds_fold)\n",
    "        mask_ok_fold = ~np.isnan(preds_fold) & ~np.isnan(y_val_fold)\n",
    "        if mask_ok_fold.sum() > 0:\n",
    "            rmse_fold = np.sqrt(mean_squared_error(y_val_fold[mask_ok_fold], preds_fold[mask_ok_fold]))\n",
    "            mae_fold = mean_absolute_error(y_val_fold[mask_ok_fold], preds_fold[mask_ok_fold])\n",
    "            gkf_rmses.append(rmse_fold)\n",
    "            gkf_maes.append(mae_fold)\n",
    "            print(f'Fold {fold_num}: RMSE {rmse_fold:.2f}, MAE {mae_fold:.2f} | Train NITs: {train_nits}, Val NITs: {val_nits}')\n",
    "\n",
    "if gkf_rmses:\n",
    "    print(f'\\n✓ GroupKFold CV Mean RMSE: {np.mean(gkf_rmses):.2f} (+/- {np.std(gkf_rmses):.2f})')\n",
    "    print(f'✓ GroupKFold CV Mean MAE: {np.mean(gkf_maes):.2f} (+/- {np.std(gkf_maes):.2f})')\n",
    "    if np.mean(gkf_rmses) > 14:\n",
    "        print('\\n WARNING: GroupKFold RMSE is significantly higher than random KFold.')\n",
    "        print('   This suggests the model struggles with unseen companies.')\n",
    "        print('   → Consider: (1) More company-level features, (2) Target transformation, (3) Sector embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb372e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 17 sector dummies\n",
      "Added profit margin from lag_1\n",
      "Added profit volatility feature\n",
      "Added 2 size category dummies\n",
      "Updated feature set: 26024 rows, 34 features (was 12)\n"
     ]
    }
   ],
   "source": [
    "df_features = df_model.copy()\n",
    "\n",
    "# A) Add sector dummies (companies in same sector may have similar patterns)\n",
    "if 'macrosector_calc' in df_features.columns:\n",
    "    sector_dummies = pd.get_dummies(df_features['macrosector_calc'], prefix='sector', drop_first=True)\n",
    "    df_features = pd.concat([df_features, sector_dummies], axis=1)\n",
    "    sector_cols = sector_dummies.columns.tolist()\n",
    "    print(f'Added {len(sector_cols)} sector dummies')\n",
    "else:\n",
    "    sector_cols = []\n",
    "\n",
    "# B) Add revenue as feature (larger companies may be more predictable)\n",
    "if 'ingresos' in df_features.columns:\n",
    "    df_features['ingresos_current'] = df_features['ingresos']\n",
    "\n",
    "# C) Add profitability ratios from lag_1 (profit margin trend)\n",
    "if 'ganancia_lag_1' in df_features.columns and 'ingresos_lag_1' in df_features.columns:\n",
    "    # Avoid division by zero\n",
    "    df_features['margin_lag_1'] = df_features['ganancia_lag_1'] / (df_features['ingresos_lag_1'].replace(0, np.nan) + 1e-3)\n",
    "    df_features['margin_lag_1'] = df_features['margin_lag_1'].fillna(0)\n",
    "    print('Added profit margin from lag_1')\n",
    "\n",
    "# D) Add volatility indicator (if company had very different profits, harder to predict)\n",
    "if 'ganancia_lag_1' in df_features.columns and 'ganancia_lag_2' in df_features.columns:\n",
    "    df_features['ganancia_volatility'] = (df_features['ganancia_lag_1'] - df_features['ganancia_lag_2']).abs()\n",
    "    df_features['ganancia_volatility'] = df_features['ganancia_volatility'].fillna(0)\n",
    "    print('Added profit volatility feature')\n",
    "\n",
    "# E) Add company size category (small/medium/large based on revenue)\n",
    "if 'ingresos' in df_features.columns:\n",
    "    df_features['size_category'] = pd.qcut(df_features['ingresos'].rank(method='first'),\n",
    "                                            q=3, labels=['small','medium','large'], duplicates='drop')\n",
    "    size_dummies = pd.get_dummies(df_features['size_category'], prefix='size', drop_first=True)\n",
    "    df_features = pd.concat([df_features, size_dummies], axis=1)\n",
    "    size_cols = size_dummies.columns.tolist()\n",
    "    print(f'Added {len(size_cols)} size category dummies')\n",
    "\n",
    "# Build updated feature list\n",
    "new_feature_cols = feature_cols.copy()\n",
    "new_feature_cols.extend(sector_cols)\n",
    "new_feature_cols.extend(['ingresos_current'])\n",
    "new_feature_cols.extend(['margin_lag_1', 'ganancia_volatility'])\n",
    "new_feature_cols.extend(size_cols)\n",
    "\n",
    "# Prepare updated X with new features\n",
    "X_new = df_features[new_feature_cols].copy()\n",
    "for col in X_new.columns:\n",
    "    if X_new[col].dtype == 'object':\n",
    "        X_new[col] = pd.to_numeric(X_new[col], errors='coerce')\n",
    "        X_new[col] = X_new[col].astype(int)\n",
    "    else:\n",
    "        X_new[col] = pd.to_numeric(X_new[col], errors='coerce')\n",
    "\n",
    "# Impute NaNs in new features\n",
    "imp_new = SimpleImputer(strategy='median')\n",
    "X_new_imputed = pd.DataFrame(imp_new.fit_transform(X_new), columns=X_new.columns)\n",
    "\n",
    "print(f'Updated feature set: {X_new_imputed.shape[0]} rows, {X_new_imputed.shape[1]} features (was {X.shape[1]})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79fef1",
   "metadata": {},
   "source": [
    "#### Train with TARGET TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f114bb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature set: 26024 rows, 32 features\n",
      "  Includes: 17 sector dummies, volatility, margin, revenue features\n",
      "\n",
      "================================================================================\n",
      "TRAINING WITH PROFIT MARGIN TARGET (GroupKFold)\n",
      "================================================================================\n",
      "Fold 1: RMSE 34.25%, MAE 5.98% | Val NITs: 2205\n",
      "Fold 2: RMSE 19.80%, MAE 5.22% | Val NITs: 2205\n",
      "Fold 3: RMSE 16.79%, MAE 5.03% | Val NITs: 2205\n",
      "Fold 4: RMSE 26.29%, MAE 5.89% | Val NITs: 2206\n",
      "Fold 5: RMSE 17.64%, MAE 5.32% | Val NITs: 2205\n",
      "\n",
      "✓ PROFIT MARGIN GroupKFold Results:\n",
      "  CV Mean RMSE: 22.95% (+/- 6.55%)\n",
      "  CV Mean MAE:  5.49% (+/- 0.38%)\n",
      "  Improvement: Std Dev 6.55 (vs 8.27 for absolute profit)\n",
      "\n",
      " MARGIN TARGET IS MORE STABLE (+20.8% improvement)\n"
     ]
    }
   ],
   "source": [
    "# 6.5) SOLUTION: Train with TARGET TRANSFORMATION (Profit Margin) + enhanced features\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Use the engineered features (with sector dummies, volatility, etc.)\n",
    "# Build comprehensive feature set\n",
    "df_final = df_model.copy()\n",
    "\n",
    "# Add sector dummies\n",
    "sector_dummies = pd.get_dummies(df_final['macrosector_calc'], prefix='sector', drop_first=True)\n",
    "df_final = pd.concat([df_final, sector_dummies], axis=1)\n",
    "sector_cols = sector_dummies.columns.tolist()\n",
    "\n",
    "# Add revenue\n",
    "df_final['ingresos_current'] = df_final['ingresos']\n",
    "\n",
    "# Add margin and volatility\n",
    "df_final['margin_lag_1'] = df_final['ganancia_lag_1'] / (df_final['ingresos_lag_1'].replace(0, np.nan) + 1e-3)\n",
    "df_final['margin_lag_1'] = df_final['margin_lag_1'].fillna(0)\n",
    "df_final['ganancia_volatility'] = (df_final['ganancia_lag_1'] - df_final['ganancia_lag_2']).abs().fillna(0)\n",
    "\n",
    "# Prepare final feature list\n",
    "final_feature_cols = feature_cols.copy() + sector_cols + ['ingresos_current', 'margin_lag_1', 'ganancia_volatility']\n",
    "\n",
    "# Prepare X with proper imputation\n",
    "X_final = df_final[final_feature_cols].copy()\n",
    "for col in X_final.columns:\n",
    "    X_final[col] = pd.to_numeric(X_final[col], errors='coerce')\n",
    "\n",
    "# Impute missing values\n",
    "imp_final = SimpleImputer(strategy='median')\n",
    "X_final_imputed = pd.DataFrame(imp_final.fit_transform(X_final), columns=X_final.columns)\n",
    "\n",
    "# Define targets\n",
    "y_absolute = pd.to_numeric(df_final['ganancias_next'], errors='coerce')\n",
    "\n",
    "y_margin = (df_final['ganancias_next'] / \n",
    "            (df_final['ingresos'].replace(0, np.nan) + 1e-3)) * 100\n",
    "y_margin = y_margin.fillna(0)\n",
    "\n",
    "print(f'\\nFeature set: {X_final_imputed.shape[0]} rows, {X_final_imputed.shape[1]} features')\n",
    "print(f'  Includes: {len(sector_cols)} sector dummies, volatility, margin, revenue features')\n",
    "\n",
    "# Train with PROFIT MARGIN target using GroupKFold\n",
    "print('\\n' + '='*80)\n",
    "print('TRAINING WITH PROFIT MARGIN TARGET (GroupKFold)')\n",
    "print('='*80)\n",
    "\n",
    "gkf_final = GroupKFold(n_splits=5)\n",
    "margin_rmses, margin_maes = [], []\n",
    "fold_num = 0\n",
    "\n",
    "for train_idx, val_idx in gkf_final.split(X_final_imputed, groups=df_final['nit'].values):\n",
    "    fold_num += 1\n",
    "    X_tr = X_final_imputed.iloc[train_idx]\n",
    "    X_val = X_final_imputed.iloc[val_idx]\n",
    "    y_tr = y_margin.values[train_idx]\n",
    "    y_val = y_margin.values[val_idx]\n",
    "    \n",
    "    val_nits = df_final.iloc[val_idx]['nit'].nunique()\n",
    "    \n",
    "    try:\n",
    "        import lightgbm as lgb\n",
    "        dtrain = lgb.Dataset(X_tr.values, label=y_tr)\n",
    "        # Tuned params: lower learning rate, higher regularization for robustness\n",
    "        params = {\n",
    "            'objective':'regression',\n",
    "            'metric':'rmse',\n",
    "            'verbosity':-1,\n",
    "            'num_leaves':15,\n",
    "            'learning_rate':0.02,\n",
    "            'reg_alpha':0.5,\n",
    "            'reg_lambda':0.5,\n",
    "            'min_data_in_leaf':10,\n",
    "            'max_depth':6\n",
    "        }\n",
    "        bst = lgb.train(params, dtrain, num_boost_round=500)\n",
    "        preds = bst.predict(X_val.values)\n",
    "    except Exception as e:\n",
    "        imp_fold = SimpleImputer(strategy='median')\n",
    "        X_tr_imp = imp_fold.fit_transform(X_tr)\n",
    "        X_val_imp = imp_fold.transform(X_val)\n",
    "        rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "        rf.fit(X_tr_imp, y_tr)\n",
    "        preds = rf.predict(X_val_imp)\n",
    "    \n",
    "    preds = np.asarray(preds)\n",
    "    mask_ok = ~np.isnan(preds) & ~np.isnan(y_val)\n",
    "    \n",
    "    if mask_ok.sum() > 0:\n",
    "        rmse = np.sqrt(mean_squared_error(y_val[mask_ok], preds[mask_ok]))\n",
    "        mae = mean_absolute_error(y_val[mask_ok], preds[mask_ok])\n",
    "        margin_rmses.append(rmse)\n",
    "        margin_maes.append(mae)\n",
    "        print(f'Fold {fold_num}: RMSE {rmse:.2f}%, MAE {mae:.2f}% | Val NITs: {val_nits}')\n",
    "\n",
    "if margin_rmses:\n",
    "    mean_rmse = np.mean(margin_rmses)\n",
    "    std_rmse = np.std(margin_rmses)\n",
    "    mean_mae = np.mean(margin_maes)\n",
    "    std_mae = np.std(margin_maes)\n",
    "    \n",
    "    print(f'\\n✓ PROFIT MARGIN GroupKFold Results:')\n",
    "    print(f'  CV Mean RMSE: {mean_rmse:.2f}% (+/- {std_rmse:.2f}%)')\n",
    "    print(f'  CV Mean MAE:  {mean_mae:.2f}% (+/- {std_mae:.2f}%)')\n",
    "    print(f'  Improvement: Std Dev {std_rmse:.2f} (vs {np.std(gkf_rmses):.2f} for absolute profit)')\n",
    "    \n",
    "    if std_rmse < np.std(gkf_rmses):\n",
    "        print(f'\\n MARGIN TARGET IS MORE STABLE (+{((np.std(gkf_rmses) - std_rmse) / np.std(gkf_rmses) * 100):.1f}% improvement)')\n",
    "    else:\n",
    "        print(f'\\n Margin still unstable; consider sector-specific models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b88b6a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's NaN values if there's missing data for the previous year:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nit</th>\n",
       "      <th>year</th>\n",
       "      <th>ganancias</th>\n",
       "      <th>ganancias_lag_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>800000118</td>\n",
       "      <td>2021</td>\n",
       "      <td>0.925267</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>800000118</td>\n",
       "      <td>2022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.925267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>800000118</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.560345</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>800000276</td>\n",
       "      <td>2021</td>\n",
       "      <td>0.925267</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>800000276</td>\n",
       "      <td>2022</td>\n",
       "      <td>0.792683</td>\n",
       "      <td>0.925267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>800000276</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.560345</td>\n",
       "      <td>0.792683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>800000276</td>\n",
       "      <td>2024</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.560345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>800000370</td>\n",
       "      <td>2021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>800000370</td>\n",
       "      <td>2022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>800000370</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         nit  year  ganancias  ganancias_lag_1\n",
       "0  800000118  2021   0.925267              NaN\n",
       "1  800000118  2022   0.000000         0.925267\n",
       "2  800000118  2023   0.560345         0.000000\n",
       "3  800000276  2021   0.925267              NaN\n",
       "4  800000276  2022   0.792683         0.925267\n",
       "5  800000276  2023   0.560345         0.792683\n",
       "6  800000276  2024   2.000000         0.560345\n",
       "7  800000370  2021   0.000000              NaN\n",
       "8  800000370  2022   0.000000         0.000000\n",
       "9  800000370  2023   0.000000         0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generar_lags_temporales(df, target_col, group_col='nit', time_col='year', lags=[1, 2]):\n",
    "    \"\"\"\n",
    "    Genera lags respetando el tiempo real. Si falta un año intermedio, devuelve NaN.\n",
    "    \"\"\"\n",
    "    df_proc = df.copy()\n",
    "    \n",
    "    # Asegurarnos que el año sea entero\n",
    "    df_proc[time_col] = df_proc[time_col].astype(int)\n",
    "    \n",
    "    # Seleccionamos solo las columnas necesarias para el merge\n",
    "    base_cols = [group_col, time_col, target_col]\n",
    "    \n",
    "    # Si hay ingresos, también les sacamos lag (muy útil para normalizar)\n",
    "    if 'ingresos' in df.columns:\n",
    "        base_cols.append('ingresos')\n",
    "\n",
    "    df_base = df_proc[base_cols].copy()\n",
    "\n",
    "    for lag in lags:\n",
    "        # 1. Preparamos el dataframe 'pasado'\n",
    "        df_shifted = df_base.copy()\n",
    "        \n",
    "        # 2. 'Movemos' el año hacia el futuro. \n",
    "        # Ejemplo: Si el dato es de 2022 y quiero usarlo como Lag 1 para 2023,\n",
    "        # le sumo 1 al año. Ahora dice \"2023\" y puedo cruzarlo con el dato real de 2023.\n",
    "        df_shifted[time_col] = df_shifted[time_col] + lag\n",
    "        \n",
    "        # 3. Renombramos las columnas\n",
    "        rename_map = {\n",
    "            target_col: f'{target_col}_lag_{lag}',\n",
    "            'ingresos': f'ingresos_lag_{lag}'\n",
    "        }\n",
    "        df_shifted = df_shifted.rename(columns=rename_map)\n",
    "        \n",
    "        # 4. Hacemos el Merge (Left Join) con el dataframe original\n",
    "        # Solo pegará el dato si existe la coincidencia exacta de NIT y AÑO\n",
    "        df_proc = pd.merge(df_proc, df_shifted, on=[group_col, time_col], how='left')\n",
    "\n",
    "    return df_proc\n",
    "\n",
    "# --- EJECUCIÓN ---\n",
    "\n",
    "# Aplicamos Lags 1 y 2 (suele ser suficiente, lag 3 o 4 pierde muchos datos)\n",
    "df_model_ready = generar_lags_temporales(df, target_col='ganancias', lags=[1, 2])\n",
    "\n",
    "print(\"There's NaN values if there's missing data for the previous year:\")\n",
    "cols_ver = ['nit', 'year', 'ganancias', 'ganancias_lag_1']\n",
    "display(df_model_ready[cols_ver].sort_values(['nit', 'year']).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55b47e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables macroeconómicas agregadas correctamente.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>PIB_Crecimiento</th>\n",
       "      <th>Tasa_Desempleo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>10.7</td>\n",
       "      <td>13.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022</td>\n",
       "      <td>7.3</td>\n",
       "      <td>11.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.6</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024</td>\n",
       "      <td>1.5</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  PIB_Crecimiento  Tasa_Desempleo\n",
       "0  2021             10.7            13.7\n",
       "1  2022              7.3            11.2\n",
       "2  2023              0.6            10.2\n",
       "6  2024              1.5            10.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "macro_data = {\n",
    "    'year': [2020, 2021, 2022, 2023, 2024],\n",
    "    'PIB_Crecimiento': [-7.0, 10.7, 7.3, 0.6, 1.5], \n",
    "    'Tasa_Desempleo': [15.9, 13.7, 11.2, 10.2, 10.5], \n",
    "    'Tasa_Interes_DTF': [2.8, 3.2, 9.8, 12.5, 9.0], \n",
    "    'TRM_Promedio': [3693, 3743, 4257, 4325, 3900]\n",
    "}\n",
    "\n",
    "df_macro = pd.DataFrame(macro_data)\n",
    "\n",
    "# Unimos estos datos a tu dataset principal\n",
    "df_enriched = pd.merge(df_model_ready, df_macro, on='year', how='left')\n",
    "\n",
    "print(\"Variables macroeconómicas agregadas correctamente.\")\n",
    "display(df_enriched[['year', 'PIB_Crecimiento', 'Tasa_Desempleo']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2937fa81",
   "metadata": {},
   "source": [
    "### Training by Sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9185e9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevas columnas creadas. Total de columnas ahora: 94\n"
     ]
    }
   ],
   "source": [
    "# 1. Crear Dummies de Sector (si no están ya)\n",
    "if 'macrosector_calc' in df_enriched.columns:\n",
    "    df_enriched = pd.get_dummies(df_enriched, columns=['macrosector_calc'], prefix='sector', dummy_na=False)\n",
    "\n",
    "# 2. Crear Interacciones (La magia para bajar el error)\n",
    "# Multiplicamos el indicador macro por el sector.\n",
    "# El modelo aprenderá: \"Si sube el interés (DTF) Y es sector Construcción = Malo para la ganancia\"\n",
    "\n",
    "sectores_cols = [c for c in df_enriched.columns if c.startswith('sector_')]\n",
    "\n",
    "for sector in sectores_cols:\n",
    "    # Interacción PIB x Sector\n",
    "    df_enriched[f'{sector}_x_PIB'] = df_enriched[sector] * df_enriched['PIB_Crecimiento']\n",
    "    \n",
    "    # Interacción Interés x Sector (Crucial para empresas endeudadas)\n",
    "    df_enriched[f'{sector}_x_DTF'] = df_enriched[sector] * df_enriched['Tasa_Interes_DTF']\n",
    "\n",
    "# 3. Crear el Target Final: MARGEN (Normalización)\n",
    "# Como discutimos, predecir el % es mejor que el valor absoluto\n",
    "df_enriched['TARGET_Margen'] = df_enriched['ganancias'] / (df_enriched['ingresos'] + 1) # +1 evita div/0\n",
    "\n",
    "print(f\"Nuevas columnas creadas. Total de columnas ahora: {df_enriched.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae6e0e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando con 78 variables: ['ingresos', 'CPI', 'ganancias_next', 'ganancia_lag_1', 'ingresos_lag_1_x', 'ganancia_lag_2', 'ingresos_lag_2_x', 'ganancia_lag_3', 'ingresos_lag_3', 'ganancia_lag_4', 'ingresos_lag_4', 'n_ganancia_lags', 'n_ingresos_lags', 'ganancia_grow_1y', 'ganancia_roll_mean_3', 'is_outlier_kde', 'ganancias_lag_1', 'ingresos_lag_1_y', 'ganancias_lag_2', 'ingresos_lag_2_y', 'PIB_Crecimiento', 'Tasa_Desempleo', 'Tasa_Interes_DTF', 'TRM_Promedio', 'sector_ADMINPUBLICA', 'sector_AGROPECUARIO', 'sector_ALIMENTICIO', 'sector_AMBIENTALES', 'sector_COMERCIO', 'sector_CONSTRUCCION', 'sector_CULTURAL', 'sector_EDUCATIVO', 'sector_ENERGETICO', 'sector_FINANCIERO', 'sector_INMOBILIARIO', 'sector_MANUFACTURA', 'sector_MINERO', 'sector_PROFESIONAL', 'sector_SALUD', 'sector_SERVICIOS', 'sector_TIC', 'sector_TRANSPORTE', 'sector_ADMINPUBLICA_x_PIB', 'sector_ADMINPUBLICA_x_DTF', 'sector_AGROPECUARIO_x_PIB', 'sector_AGROPECUARIO_x_DTF', 'sector_ALIMENTICIO_x_PIB', 'sector_ALIMENTICIO_x_DTF', 'sector_AMBIENTALES_x_PIB', 'sector_AMBIENTALES_x_DTF', 'sector_COMERCIO_x_PIB', 'sector_COMERCIO_x_DTF', 'sector_CONSTRUCCION_x_PIB', 'sector_CONSTRUCCION_x_DTF', 'sector_CULTURAL_x_PIB', 'sector_CULTURAL_x_DTF', 'sector_EDUCATIVO_x_PIB', 'sector_EDUCATIVO_x_DTF', 'sector_ENERGETICO_x_PIB', 'sector_ENERGETICO_x_DTF', 'sector_FINANCIERO_x_PIB', 'sector_FINANCIERO_x_DTF', 'sector_INMOBILIARIO_x_PIB', 'sector_INMOBILIARIO_x_DTF', 'sector_MANUFACTURA_x_PIB', 'sector_MANUFACTURA_x_DTF', 'sector_MINERO_x_PIB', 'sector_MINERO_x_DTF', 'sector_PROFESIONAL_x_PIB', 'sector_PROFESIONAL_x_DTF', 'sector_SALUD_x_PIB', 'sector_SALUD_x_DTF', 'sector_SERVICIOS_x_PIB', 'sector_SERVICIOS_x_DTF', 'sector_TIC_x_PIB', 'sector_TIC_x_DTF', 'sector_TRANSPORTE_x_PIB', 'sector_TRANSPORTE_x_DTF']\n",
      "--- Entrenando: Expert_Servicios ---\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid_0's l1: 0.0287038\n",
      "MAE Final (Expert_Servicios): 0.0287 (Margen de Ganancia)\n",
      "--- Entrenando: Expert_Resto ---\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[108]\tvalid_0's l1: 0.0306732\n",
      "MAE Final (Expert_Resto): 0.0307 (Margen de Ganancia)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "df_trainable = df_enriched.copy()\n",
    "\n",
    "# 1. CORRECCIÓN: Filtrar solo columnas numéricas para evitar el error de tipos 'object'\n",
    "# -------------------------------------------------------------------------\n",
    "numeric_features = df_trainable.select_dtypes(include=['number', 'bool']).columns.tolist()\n",
    "exclude_cols = ['nit', 'razon_social', 'ganancias', 'TARGET_Margen', 'year']\n",
    "features = [c for c in numeric_features if c not in exclude_cols]\n",
    "\n",
    "print(f\"Entrenando con {len(features)} variables: {features}\")\n",
    "\n",
    "# 2. Función de Entrenamiento (Misma lógica, datos limpios)\n",
    "# -------------------------------------------------------------------------\n",
    "def entrenar_modelo_sectorial(df_subset, nombre_modelo):\n",
    "    if df_subset.empty:\n",
    "        print(f\"No hay datos para {nombre_modelo}\")\n",
    "        return None\n",
    "        \n",
    "    X = df_subset[features]\n",
    "    y = df_subset['TARGET_Margen']\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Dataset LGBM\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mae',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'feature_fraction': 0.9,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    print(f\"--- Entrenando: {nombre_modelo} ---\")\n",
    "    gbm = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[test_data],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(False)] # False para menos ruido\n",
    "    )\n",
    "    \n",
    "    # Métrica final\n",
    "    y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(f\"MAE Final ({nombre_modelo}): {mae:.4f} (Margen de Ganancia)\")\n",
    "    \n",
    "    return gbm\n",
    "df_servicios = df_trainable[df_trainable['sector_SERVICIOS'] == 1]\n",
    "\n",
    "df_resto     = df_trainable[df_trainable['sector_SERVICIOS'] == 0]\n",
    "\n",
    "# 3. Entrenar los modelos corregidos\n",
    "modelo_servicios = entrenar_modelo_sectorial(df_servicios, \"Expert_Servicios\")\n",
    "modelo_general = entrenar_modelo_sectorial(df_resto, \"Expert_Resto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd6ca2",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa48dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Preparar datos del ÚLTIMO año disponible para cada empresa\n",
    "df_last = df_enriched.sort_values('Año de Corte').groupby('NIT').tail(1).copy()\n",
    "\n",
    "# 2. Simular escenario macroeconómico 2025\n",
    "df_last['PIB_Crecimiento'] = 3.0\n",
    "df_last['Tasa_Interes_DTF'] = 8.0\n",
    "\n",
    "# Recalcular interacciones con los nuevos datos macro\n",
    "sectores_cols = [c for c in df_last.columns if c.startswith('sector_')]\n",
    "for sector in sectores_cols:\n",
    "    df_last[f'{sector}_x_PIB'] = df_last[sector] * df_last['PIB_Crecimiento']\n",
    "    df_last[f'{sector}_x_DTF'] = df_last[sector] * df_last['Tasa_Interes_DTF']\n",
    "\n",
    "# 3. Separar por sector y predecir\n",
    "mask_last_servicios = df_last['sector_SERVICIOS'] == 1\n",
    "df_last_servicios = df_last[mask_last_servicios]\n",
    "df_last_resto = df_last[~mask_last_servicios]\n",
    "\n",
    "df_last['Prediccion_Margen'] = 0.0\n",
    "\n",
    "if modelo_servicios and not df_last_servicios.empty:\n",
    "    # Asegurarnos de usar solo las features numéricas correctas\n",
    "    preds_serv = modelo_servicios.predict(df_last_servicios[features])\n",
    "    df_last.loc[mask_last_servicios, 'Prediccion_Margen'] = preds_serv\n",
    "\n",
    "if modelo_general and not df_last_resto.empty:\n",
    "    preds_gen = modelo_general.predict(df_last_resto[features])\n",
    "    df_last.loc[~mask_last_servicios, 'Prediccion_Margen'] = preds_gen\n",
    "\n",
    "# 4. Calcular Ganancia Proyectada\n",
    "df_last['Ganancia_Proyectada_Mils'] = df_last['Prediccion_Margen'] * df_last['INGRESOS OPERACIONALES']\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. CORRECCIÓN: RECUPERAR COLUMNAS DE TEXTO (MACROSECTOR, REGIÓN)\n",
    "# ==============================================================================\n",
    "# Identificamos qué columnas faltan en df_last para el reporte final\n",
    "cols_necesarias = ['MACROSECTOR', 'REGIÓN']\n",
    "cols_faltantes = [c for c in cols_necesarias if c not in df_last.columns]\n",
    "\n",
    "if cols_faltantes:\n",
    "    print(f\"Recuperando columnas faltantes del dataset original: {cols_faltantes}\")\n",
    "    # Tomamos las columnas del df original (df), asegurando un registro único por NIT\n",
    "    df_info_original = df[['NIT'] + cols_faltantes].drop_duplicates(subset='NIT')\n",
    "    \n",
    "    # Hacemos merge para traerlas de vuelta a df_last\n",
    "    df_last = pd.merge(df_last, df_info_original, on='NIT', how='left')\n",
    "\n",
    "# 6. Exportar resultados para el Dashboard\n",
    "cols_export = ['NIT', 'RAZÓN SOCIAL', 'MACROSECTOR', 'REGIÓN', 'INGRESOS OPERACIONALES', \n",
    "               'GANANCIA_REAL', 'Prediccion_Margen', 'Ganancia_Proyectada_Mils']\n",
    "\n",
    "df_last[cols_export].to_csv('resultados_predicciones_2025.csv', index=False)\n",
    "\n",
    "print(\"¡Éxito! Archivo 'resultados_predicciones_2025.csv' generado correctamente.\")\n",
    "display(df_last[['RAZÓN SOCIAL', 'MACROSECTOR', 'Prediccion_Margen']].sort_values('Prediccion_Margen', ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Definir las empresas y sus predicciones (lo que obtuviste)\n",
    "datos_validacion = {\n",
    "    'nit': [\n",
    "        890903938, # SURAMERICANA S.A. (Ajustar NIT si difiere)\n",
    "        890980051, # GRUPO SURA\n",
    "        890900050, # NUTRESA\n",
    "        900123456, # GRUPO TRINITY (Ejemplo, ajusta al NIT real de tu df)\n",
    "        890900266  # GRUPO ARGOS\n",
    "    ],\n",
    "    'razon_social': [\n",
    "        'SURAMERICANA S.A.',\n",
    "        'GRUPO DE INV. SURAMERICANA',\n",
    "        'GRUPO NUTRESA S.A',\n",
    "        'GRUPO TRINITY SAS',\n",
    "        'GRUPO ARGOS S.A.'\n",
    "    ],\n",
    "    'Prediccion_2025': [2.06, 1.78, 1.63, 1.36, 1.26] # Tus resultados\n",
    "}\n",
    "\n",
    "df_preds = pd.DataFrame(datos_validacion)\n",
    "\n",
    "# 2. Calcular el histórico real desde tu dataset original (df)\n",
    "# Filtramos por nombre aproximado para asegurar coincidencia\n",
    "nombres_clave = ['SURAMERICANA', 'NUTRESA', 'TRINITY', 'ARGOS']\n",
    "mask = df['RAZÓN SOCIAL'].str.contains('|'.join(nombres_clave), case=False, na=False)\n",
    "df_hist = df[mask].copy()\n",
    "\n",
    "# Calculamos el margen real histórico\n",
    "df_hist['Margen_Real_Pct'] = (df_hist['GANANCIA (PÉRDIDA)'] / df_hist['INGRESOS OPERACIONALES']) * 100\n",
    "\n",
    "# Agrupamos para obtener el promedio histórico por empresa\n",
    "df_hist_avg = df_hist.groupby('RAZÓN SOCIAL')['Margen_Real_Pct'].mean().reset_index()\n",
    "\n",
    "# 3. Cruzar los datos (Manual para coincidencia visual rápida)\n",
    "# Nota: Para tu tesis, asegúrate de cruzar por NIT exacto.\n",
    "# Aquí creamos una tabla comparativa visual simulada basada en tus empresas\n",
    "df_plot = pd.DataFrame({\n",
    "    'Empresa': df_preds['RAZÓN SOCIAL'],\n",
    "    'Margen Promedio (2021-2023)': [2.5, 3.1, 2.8, 1.9, 4.5], # EJEMPLO: Reemplaza esto con los valores reales de df_hist_avg\n",
    "    'Predicción Modelo (2025)': df_preds['Prediccion_2025']\n",
    "})\n",
    "\n",
    "# Transformar para graficar (Melt)\n",
    "df_melt = df_plot.melt(id_vars='Empresa', var_name='Periodo', value_name='Margen %')\n",
    "\n",
    "# 4. Graficar\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_melt, x='Empresa', y='Margen %', hue='Periodo', palette=['#1f77b4', '#ff7f0e'])\n",
    "plt.title('Validación de Realismo: Histórico vs. Predicción Modelo', fontsize=14)\n",
    "plt.ylabel('Margen de Ganancia (%)')\n",
    "plt.xticks(rotation=15)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Añadir etiquetas de valor\n",
    "for p in plt.gca().patches:\n",
    "    plt.gca().annotate(f'{p.get_height():.2f}%', \n",
    "                       (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                       ha='center', va='bottom', fontsize=10, color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
